{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg",
    "colab_type": "text"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoRP98MpR-qj",
    "colab_type": "text"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT",
    "colab_type": "text"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country,Age,Salary,Purchased\n",
      "\n",
      "[['France', '44', '72000', 'No'], ['Spain', '27', '48000', 'Yes'], ['Germany', '30', '54000', 'No'], ['Spain', '38', '61000', 'No'], ['Germany', '40', '', 'Yes'], ['France', '35', '58000', 'Yes'], ['Spain', '', '52000', 'No'], ['France', '48', '79000', 'Yes'], ['Germany', '50', '83000', 'No'], ['France', '37', '67000', 'Yes']]\n"
     ]
    }
   ],
   "source": [
    "with open('Data.csv') as file:\n",
    "    file_iter = iter(file)\n",
    "    headers = next(file_iter)\n",
    "    data_row = []\n",
    "    for line in file_iter:\n",
    "        tem = line.strip('\\n').split(',')\n",
    "        data_row.append(tem)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
      "[['No']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['No']\n",
      " ['Yes']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['Yes']]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('data.csv')\n",
    "#print(dataset)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "print(X)\n",
    "Y = dataset.iloc[:, -1].values\n",
    "print(Y)\n",
    "Y = Y.reshape(10,1)\n",
    "print(Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "[['No']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['No']\n",
      " ['Yes']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['Yes']]\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv('data.csv' )\n",
    "# Set independent variables or features.\n",
    "X = dataset.iloc[:,:-1].values  #Locate indexes [rows, columns]\n",
    "\n",
    "# Set dependent variable.\n",
    "Y = dataset.iloc[:,-1:].values\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(Y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Purchased\n",
      "0        No\n",
      "1       Yes\n",
      "2        No\n",
      "3        No\n",
      "4       Yes\n",
      "5       Yes\n",
      "6        No\n",
      "7       Yes\n",
      "8        No\n",
      "9       Yes\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC",
    "colab_type": "text"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 63777.77777777778]\n",
      " [35.0 58000.0]\n",
      " [38.77777777777778 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print(X[:, 1:3])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK",
    "colab_type": "text"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh",
    "colab_type": "text"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('encoder', OneHotEncoder(), [0])])\n",
      "Applies transformers to columns of an array or pandas DataFrame.\n",
      "\n",
      "    This estimator allows different columns or column subsets of the input\n",
      "    to be transformed separately and the features generated by each transformer\n",
      "    will be concatenated to form a single feature space.\n",
      "    This is useful for heterogeneous or columnar data, to combine several\n",
      "    feature extraction mechanisms or transformations into a single transformer.\n",
      "\n",
      "    Read more in the :ref:`User Guide <column_transformer>`.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    transformers : list of tuples\n",
      "        List of (name, transformer, columns) tuples specifying the\n",
      "        transformer objects to be applied to subsets of the data.\n",
      "\n",
      "        name : str\n",
      "            Like in Pipeline and FeatureUnion, this allows the transformer and\n",
      "            its parameters to be set using ``set_params`` and searched in grid\n",
      "            search.\n",
      "        transformer : {'drop', 'passthrough'} or estimator\n",
      "            Estimator must support :term:`fit` and :term:`transform`.\n",
      "            Special-cased strings 'drop' and 'passthrough' are accepted as\n",
      "            well, to indicate to drop the columns or to pass them through\n",
      "            untransformed, respectively.\n",
      "        columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n",
      "            Indexes the data on its second axis. Integers are interpreted as\n",
      "            positional columns, while strings can reference DataFrame columns\n",
      "            by name.  A scalar string or int should be used where\n",
      "            ``transformer`` expects X to be a 1d array-like (vector),\n",
      "            otherwise a 2d array will be passed to the transformer.\n",
      "            A callable is passed the input data `X` and can return any of the\n",
      "            above. To select multiple columns by name or dtype, you can use\n",
      "            :obj:`make_column_selector`.\n",
      "\n",
      "    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n",
      "        By default, only the specified columns in `transformers` are\n",
      "        transformed and combined in the output, and the non-specified\n",
      "        columns are dropped. (default of ``'drop'``).\n",
      "        By specifying ``remainder='passthrough'``, all remaining columns that\n",
      "        were not specified in `transformers` will be automatically passed\n",
      "        through. This subset of columns is concatenated with the output of\n",
      "        the transformers.\n",
      "        By setting ``remainder`` to be an estimator, the remaining\n",
      "        non-specified columns will use the ``remainder`` estimator. The\n",
      "        estimator must support :term:`fit` and :term:`transform`.\n",
      "        Note that using this feature requires that the DataFrame columns\n",
      "        input at :term:`fit` and :term:`transform` have identical order.\n",
      "\n",
      "    sparse_threshold : float, default=0.3\n",
      "        If the output of the different transformers contains sparse matrices,\n",
      "        these will be stacked as a sparse matrix if the overall density is\n",
      "        lower than this value. Use ``sparse_threshold=0`` to always return\n",
      "        dense.  When the transformed output consists of all dense data, the\n",
      "        stacked result will be dense, and this keyword will be ignored.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        Number of jobs to run in parallel.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    transformer_weights : dict, default=None\n",
      "        Multiplicative weights for features per transformer. The output of the\n",
      "        transformer is multiplied by these weights. Keys are transformer names,\n",
      "        values the weights.\n",
      "\n",
      "    verbose : bool, default=False\n",
      "        If True, the time elapsed while fitting each transformer will be\n",
      "        printed as it is completed.\n",
      "\n",
      "    verbose_feature_names_out : bool, default=True\n",
      "        If True, :meth:`get_feature_names_out` will prefix all feature names\n",
      "        with the name of the transformer that generated that feature.\n",
      "        If False, :meth:`get_feature_names_out` will not prefix any feature\n",
      "        names and will error if feature names are not unique.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    transformers_ : list\n",
      "        The collection of fitted transformers as tuples of\n",
      "        (name, fitted_transformer, column). `fitted_transformer` can be an\n",
      "        estimator, 'drop', or 'passthrough'. In case there were no columns\n",
      "        selected, this will be the unfitted transformer.\n",
      "        If there are remaining columns, the final element is a tuple of the\n",
      "        form:\n",
      "        ('remainder', transformer, remaining_columns) corresponding to the\n",
      "        ``remainder`` parameter. If there are remaining columns, then\n",
      "        ``len(transformers_)==len(transformers)+1``, otherwise\n",
      "        ``len(transformers_)==len(transformers)``.\n",
      "\n",
      "    named_transformers_ : :class:`~sklearn.utils.Bunch`\n",
      "        Read-only attribute to access any transformer by given name.\n",
      "        Keys are transformer names and values are the fitted transformer\n",
      "        objects.\n",
      "\n",
      "    sparse_output_ : bool\n",
      "        Boolean flag indicating whether the output of ``transform`` is a\n",
      "        sparse matrix or a dense numpy array, which depends on the output\n",
      "        of the individual transformers and the `sparse_threshold` keyword.\n",
      "\n",
      "    output_indices_ : dict\n",
      "        A dictionary from each transformer name to a slice, where the slice\n",
      "        corresponds to indices in the transformed output. This is useful to\n",
      "        inspect which transformer is responsible for which transformed\n",
      "        feature(s).\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`. Only defined if the\n",
      "        underlying transformers expose such an attribute when fit.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    make_column_transformer : Convenience function for\n",
      "        combining the outputs of multiple transformer objects applied to\n",
      "        column subsets of the original feature space.\n",
      "    make_column_selector : Convenience function for selecting\n",
      "        columns based on datatype or the columns name with a regex pattern.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The order of the columns in the transformed feature matrix follows the\n",
      "    order of how the columns are specified in the `transformers` list.\n",
      "    Columns of the original feature matrix that are not specified are\n",
      "    dropped from the resulting transformed feature matrix, unless specified\n",
      "    in the `passthrough` keyword. Those columns specified with `passthrough`\n",
      "    are added at the right to the output of the transformers.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.compose import ColumnTransformer\n",
      "    >>> from sklearn.preprocessing import Normalizer\n",
      "    >>> ct = ColumnTransformer(\n",
      "    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
      "    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
      "    >>> X = np.array([[0., 1., 2., 2.],\n",
      "    ...               [1., 1., 0., 1.]])\n",
      "    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n",
      "    >>> # is applied for the two first and two last elements of each\n",
      "    >>> # row independently.\n",
      "    >>> ct.fit_transform(X)\n",
      "    array([[0. , 1. , 0.5, 0.5],\n",
      "           [0.5, 0.5, 0. , 1. ]])\n",
      "\n",
      "    :class:`ColumnTransformer` can be configured with a transformer that requires\n",
      "    a 1d array by setting the column to a string:\n",
      "\n",
      "    >>> from sklearn.feature_extraction import FeatureHasher\n",
      "    >>> from sklearn.preprocessing import MinMaxScaler\n",
      "    >>> import pandas as pd   # doctest: +SKIP\n",
      "    >>> X = pd.DataFrame({\n",
      "    ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n",
      "    ...     \"width\": [3, 4, 5],\n",
      "    ... })  # doctest: +SKIP\n",
      "    >>> # \"documents\" is a string which configures ColumnTransformer to\n",
      "    >>> # pass the documents column as a 1d array to the FeatureHasher\n",
      "    >>> ct = ColumnTransformer(\n",
      "    ...     [(\"text_preprocess\", FeatureHasher(input_type=\"string\"), \"documents\"),\n",
      "    ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n",
      "    >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n",
      "    \n",
      "[[1.0 0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough', )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6",
    "colab_type": "text"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FyhY8-gPpFCa",
    "colab_type": "code",
    "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "print(y)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_vcgm3qZKW",
    "colab_type": "text"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GuwQhFdKrYTM",
    "colab_type": "code",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    }
   },
   "source": [
    "print(X_train)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TUrX_Tvcrbi4",
    "colab_type": "code",
    "outputId": "9a041a9b-2642-4828-fa2f-a431d7d77631",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "print(X_test)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pSMHiIsWreQY",
    "colab_type": "code",
    "outputId": "5afe91e0-9244-4bf5-ec1b-e3e092b85c08",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "print(y_train)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I_tW7H56rgtW",
    "colab_type": "code",
    "outputId": "2a93f141-2a99-4a69-eec5-c82a3bb8d36b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "print(y_test)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGqbS4TqkIR",
    "colab_type": "text"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DWPET8ZdlMnu",
    "colab_type": "code",
    "outputId": "dea86927-5124-4e2a-e974-2804df9a913c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    }
   },
   "source": [
    "print(X_train)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sTXykB_QlRjE",
    "colab_type": "code",
    "outputId": "b68f0cfc-d07c-48cb-80d0-6800028c41f9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "print(X_test)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
